# AI Agent with Retrieval-Augmented Generation (RAG) on Azure

This project implements an AI Agent that can intelligently answer user queries by either responding directly or retrieving relevant information from internal documents using a Retrieval-Augmented Generation (RAG) approach.

The application is built using **FastAPI**, follows a modular AI-agent design, and is deployed on **Azure App Service** with a publicly accessible endpoint.

---

## 1. Problem Statement

The goal of this project is to build an AI agent that:

- Accepts a user query
- Decides whether the query can be answered directly or requires document-based retrieval
- Retrieves relevant information from internal documents when needed
- Returns a clear, structured response with source references
- Is deployed and accessible via a public Azure URL

This setup simulates a real-world use case such as answering questions about **company policies, internal documentation, or FAQs**.

---

## 2. Architecture Overview

### High-Level Flow

User
↓
FastAPI (/ask endpoint)
↓
AI Agent (Decision Logic)
├─ Direct Response
└─ Retrieval-Augmented Generation (RAG)
├─ Document Loading
├─ Retrieval Logic
└─ Context Injection
↓
Final Answer + Source Documents


### Core Components

- **AI Agent**  
  Determines whether a query requires document retrieval or can be handled directly.

- **RAG Module**  
  Retrieves relevant document content and injects it into the response.

- **Session Memory**  
  Maintains basic session-level conversation history using a `session_id`.

- **FastAPI Backend**  
  Exposes REST APIs for interaction.

- **Azure App Service**  
  Hosts the application and provides a public endpoint.

---

## 3. Tech Stack

- **Language**: Python 3.10  
- **Backend Framework**: FastAPI  
- **AI / Agent Logic**: Custom rule-based agent (LLM-ready design)  
- **Retrieval System**: In-memory document retrieval  
- **Deployment**: Azure App Service (Linux, Free Tier F1)  
- **Web Server**: Gunicorn  
- **Version Control**: Git + GitHub  

---

## 4. API Specification

### Endpoint



POST /ask


### Request Body

```json
{
  "query": "string",
  "session_id": "optional"
}

Response Format
{
  "answer": "string",
  "source": ["doc1.txt", "doc2.txt"]
}


answer: Final response generated by the agent

source: List of documents used to generate the answer (empty if not applicable)

5. Project Structure
ai-agent-rag/
│
├── app/
│   ├── main.py          # FastAPI entry point
│   ├── agent.py        # Agent decision logic
│   ├── rag.py          # Retrieval logic
│   ├── embeddings.py   # Embedding / retrieval helpers
│   ├── memory.py       # Session memory
│   ├── config.py       # Configuration helpers
│   └── documents/      # Sample internal documents
│
├── requirements.txt
├── README.md
└── .gitignore

6. Local Setup Instructions
Clone Repository
git clone https://github.com/nextrack17/ai-agent-rag.git
cd ai-agent-rag

Create Virtual Environment
python -m venv venv
venv\Scripts\activate   # Windows

Install Dependencies
pip install -r requirements.txt

Run Locally
uvicorn app.main:app --reload


Open in browser:

http://127.0.0.1:8000/docs

7. Azure Deployment
Deployment Platform

Azure App Service

OS: Linux

Runtime: Python 3.10

Plan: Free Tier (F1)

Deployment Steps (Summary)

Create an Azure App Service

Connect the GitHub repository via Deployment Center

Set the startup command:

   gunicorn app.main:app -k uvicorn.workers.UvicornWorker --workers 1 --timeout 120


Restart the app service

Live URLs

Base URL

https://ai-agent-rag-nextrack-h7fyfpfzajf7bgad.southindia-01.azurewebsites.net/


Swagger UI

https://ai-agent-rag-nextrack-h7fyfpfzajf7bgad.southindia-01.azurewebsites.net/docs

8. Design Decisions
Agent Routing Logic

Implemented deterministic routing logic to decide when document retrieval is required.

Keeps the agent behavior transparent, explainable, and easy to evaluate.

Retrieval Strategy (Important Note)

FAISS was used during local development for vector-based retrieval.

During Azure deployment (Free F1 tier), FAISS caused native dependency and runtime stability issues.

To ensure deployment reliability, FAISS was removed from the deployed version and replaced with a lightweight in-memory retrieval mechanism.

This trade-off prioritizes system stability and availability while still demonstrating correct RAG architecture and flow.

Session Memory

Simple in-memory session tracking using session_id.

Satisfies the requirement for basic agent memory without external stateful services.

9. Limitations

No authentication or authorization

In-memory retrieval (not optimized for large document sets)

Session memory resets on app restart

LLM-based reasoning can be further improved

10. Future Improvements

Re-enable FAISS or Azure AI Search on a paid Azure plan

Integrate Azure OpenAI embeddings and chat completions

Add persistent memory (Redis / Cosmos DB)

Improve agent decision-making using LLM-based tool selection

Add monitoring and logging via Azure Monitor

11. Author

Divyanshu Pandey
